{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ddb2e9",
   "metadata": {},
   "source": [
    "## Check pySPM version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6403df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.2\n"
     ]
    }
   ],
   "source": [
    "import pySPM\n",
    "print(pySPM.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spmpy import SPMFile\n",
    "spm_file = SPMFile('C:/Users/cobia/OneDrive - University of Cambridge/HF_Database/AFM/raw/short_end_01.0_00000.spm')\n",
    "spm_file.header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe373321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cobia\\anaconda3\\envs\\pyspm\\Lib\\site-packages\\spmpy\\spmloader.py:32: UserWarning: Untested SPM file verison, calculations may be inaccurate: 0x09700108. Supported versions; ['0x09200201', '0x09400202', '0x09400103']\n",
      "  warnings.warn(f'Untested SPM file verison, calculations may be inaccurate: {file_version}. '\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pint import Quantity\n",
    "from spmpy.ciaoparams import ScaleParameter, ValueParameter, SelectParameter\n",
    "from spmpy import SPMFile\n",
    "import os\n",
    "\n",
    "def custom_serializer(obj):\n",
    "    \"\"\"\n",
    "    Convert problematic objects into JSON-serializable representations.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- NumPy arrays ---\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "\n",
    "    # --- Pint Quantity ---\n",
    "    if isinstance(obj, Quantity):\n",
    "        return {\"value\": obj.magnitude, \"unit\": str(obj.units)}\n",
    "\n",
    "    # --- spmpy parameters ---\n",
    "    if isinstance(obj, (ScaleParameter, ValueParameter, SelectParameter)):\n",
    "        # Try to capture core fields\n",
    "        d = {\"_type\": obj.__class__.__name__}\n",
    "        if hasattr(obj, \"value\"):\n",
    "            d[\"value\"] = obj.value\n",
    "        if hasattr(obj, \"unit\"):\n",
    "            d[\"unit\"] = str(obj.unit)\n",
    "        if hasattr(obj, \"choices\"):  # for SelectParameter\n",
    "            d[\"choices\"] = obj.choices\n",
    "        return d\n",
    "\n",
    "    # --- Fallback for other objects ---\n",
    "    try:\n",
    "        return str(obj)\n",
    "    except Exception:\n",
    "        return f\"<<unserializable: {type(obj).__name__}>>\"\n",
    "\n",
    "# Define the folder containing .spm files\n",
    "folder_path = r\"C:\\Users\\cobia\\OneDrive - University of Cambridge\\HF_Database\\AFM\\raw\"\n",
    "output_path = r\"C:\\Users\\cobia\\OneDrive - University of Cambridge\\HF_Database\\AFM\\metadata\"\n",
    "\n",
    "spm_files = [f for f in os.listdir(folder_path) if f.endswith('.spm')]\n",
    "\n",
    "for spm_filename in spm_files:\n",
    "    basename = os.path.splitext(spm_filename)[0]\n",
    "    spm_file = SPMFile(os.path.join(folder_path, spm_filename))\n",
    "    json_str = json.dumps(spm_file.header, default=custom_serializer, indent=2)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_path, basename +'.json'), \"w\") as f:\n",
    "        json.dump(spm_file.header, f, default=custom_serializer, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced12315",
   "metadata": {},
   "source": [
    "## Get list of SPM files in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder containing .spm files\n",
    "folder_path = r\"C:\\Users\\cobia\\OneDrive - University of Cambridge\\HF_Database\\AFM\\raw\"\n",
    "\n",
    "spm_files = [f for f in os.listdir(folder_path) if f.endswith('.spm')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea9bcb",
   "metadata": {},
   "source": [
    "## Import all .spm files and preprocess by (Skip if data already corrected and using Lumispy kernel):\n",
    "- Align rows by median of differences\n",
    "- Filter: scar removal\n",
    "- Level data by plane subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37409c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pySPM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from IPython import display\n",
    "\n",
    "from skimage.morphology import binary_erosion, disk\n",
    "\n",
    "import copy\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "from skimage import exposure\n",
    "\n",
    "\n",
    "\n",
    "# Define the folder containing .spm files\n",
    "folder_path = r\"C:\\Users\\cobia\\OneDrive - University of Cambridge\\HF_Database\\AFM\\raw\"\n",
    "output_path = r\"C:\\Users\\cobia\\OneDrive - University of Cambridge\\HF_Database\\AFM\\processed\"\n",
    "\n",
    "spm_files = [f for f in os.listdir(folder_path) if f.endswith('.spm')]\n",
    "\n",
    "fig, ax = plt.subplots(math.ceil(len(spm_files)/3), 3, figsize=(10, 4*math.ceil(len(spm_files)/3)))\n",
    "\n",
    "# Loop through each file\n",
    "for idx, filename in enumerate(spm_files):\n",
    "    filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.exists(os.path.join(output_path, f\"{filename}_corrected.csv\")):\n",
    "        print(f\"Processed file already exists for {filename}\")\n",
    "        topoD = np.loadtxt(os.path.join(output_path, f\"{filename}_corrected.csv\"), delimiter=',')\n",
    "        ax[int(idx/3), idx%3].imshow(topoD, cmap='afmhot')\n",
    "        ax[int(idx/3), idx%3].set_title(filename)\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Processing file: {filename}\")    \n",
    "    \n",
    "    ScanB = pySPM.Bruker(filepath)\n",
    "\n",
    "    # Try to get the first data channel (usually '0/data')\n",
    "\n",
    "    data_channel = ScanB.get_channel(\"Height\")\n",
    "\n",
    "    # topo2 = data_channel.correct_plane(inline=False)\n",
    "\n",
    "    topo2 = copy.deepcopy(data_channel)\n",
    "    # topo2.correct_median_diff()\n",
    "\n",
    "    pixels = topo2.pixels.copy()\n",
    "    for row in range(pixels.shape[0]):\n",
    "        p = np.polyfit(np.arange(pixels.shape[1]), pixels[row, :], deg=2)\n",
    "        pixels[row, :] -= np.polyval(p, np.arange(pixels.shape[1]))\n",
    "\n",
    "    topo2.pixels = pixels\n",
    "\n",
    "    topoD = topo2.filter_scars_removal(.7, inline=False)\n",
    "\n",
    "    # # Correct the plane and apply filtering\n",
    "    # topoD = topo3.corr_fit2d(inline=False)\n",
    "    # topoD = topoD.filter_scars_removal()\n",
    "\n",
    "    topoD.show(ax=ax[int(idx/3), idx%3], cmap='afmhot', title=filename)\n",
    "\n",
    "    np.save(os.path.join(output_path, f\"{filename}_corrected.npy\"), topoD.pixels)\n",
    "\n",
    "    # Scale AFM data to 0â€“255 and convert to uint8\n",
    "    # img_scaled = exposure.rescale_intensity(\n",
    "    #     topoD.pixels, out_range=(0, 255)\n",
    "    # ).astype(np.uint8)\n",
    "\n",
    "    # # Save as grayscale PNG\n",
    "    # imageio.imwrite(\n",
    "    #     os.path.join(folder_path, f\"{filename}_corrected.png\"),\n",
    "    #     img_scaled\n",
    "    # )\n",
    "\n",
    "    # # Extract the height data\n",
    "    # Z = data_channel\n",
    "\n",
    "    # # Plot the height map\n",
    "    # plt.figure(figsize=(6, 5))\n",
    "    # plt.imshow(Z, cmap='afmhot', origin='lower')\n",
    "    # plt.colorbar(label='Height')\n",
    "    # plt.title(f'Height map: {filename}')\n",
    "    # plt.xlabel('X')\n",
    "    # plt.ylabel('Y')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991771e4",
   "metadata": {},
   "source": [
    "## Import manual labels for each AFM scan and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, Patch\n",
    "from math import sqrt\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Prepare figure layout\n",
    "n_images = len(spm_files)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_images / n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 14 / n_cols * n_rows))\n",
    "axes = axes.flatten() if n_images > 1 else [axes]\n",
    "\n",
    "for i, filename in enumerate(spm_files):\n",
    "    afm_file = os.path.join(folder_path, f\"{filename}_corrected.csv\")\n",
    "    label_file = os.path.join(folder_path, f\"{os.path.splitext(filename)[0]}_categorised.csv\")\n",
    "\n",
    "    afm_data = pd.read_csv(afm_file, header=None).values\n",
    "    labels = pd.read_csv(label_file)\n",
    "\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(afm_data, cmap='afmhot', vmin=-7, vmax=3)\n",
    "    ax.set_title(filename, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Add colorbar to each subplot\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Height (nm)', fontsize=8)\n",
    "\n",
    "    # Calculate average size for large and small categories\n",
    "    avg_large = labels[labels['category'] == 'large']['size'].mean()\n",
    "    avg_small = labels[labels['category'] == 'small']['size'].mean()\n",
    "\n",
    "    # Add labeled circles\n",
    "    for _, row in labels.iterrows():\n",
    "        if row['category'] not in ['large', 'small']:\n",
    "            continue\n",
    "        color = 'red' if row['category'] == 'large' else 'orange'\n",
    "        if pd.isna(row['size']):\n",
    "            if row['category'] == 'large':\n",
    "                radius = sqrt(avg_large)\n",
    "            else:\n",
    "                radius = sqrt(avg_small)\n",
    "        else:\n",
    "            radius = sqrt(row['size'])\n",
    "        circle = Circle((row['x'], afm_data.shape[0] - 1 - row['y']), radius, color=color, fill=False, linewidth=1.5)\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Add global legend\n",
    "legend_elements = [\n",
    "    Patch(edgecolor='red', facecolor='none', label='Large', linewidth=2),\n",
    "    Patch(edgecolor='orange', facecolor='none', label='Small', linewidth=2)\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=2, frameon=False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])  # space for legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccc6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0601e42",
   "metadata": {},
   "source": [
    "## Display AFM data without manual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba588f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare figure layout\n",
    "n_images = len(spm_files)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_images / n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 14 / n_cols * n_rows))\n",
    "axes = axes.flatten() if n_images > 1 else [axes]\n",
    "\n",
    "for i, filename in enumerate(spm_files):\n",
    "    afm_file = os.path.join(folder_path, f\"{filename}_corrected.csv\")\n",
    "    label_file = os.path.join(folder_path, f\"{os.path.splitext(filename)[0]}_categorised.csv\")\n",
    "\n",
    "    afm_data = pd.read_csv(afm_file, header=None).values\n",
    "    labels = pd.read_csv(label_file)\n",
    "\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(afm_data, cmap='afmhot', vmin=-7, vmax=3)\n",
    "    ax.set_title(filename, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647338b2",
   "metadata": {},
   "source": [
    "## Use local tresholding to find spots and visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee087db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import filters, measure\n",
    "\n",
    "# Prepare figure layout\n",
    "n_images = len(spm_files)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_images / n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10 / n_cols * n_rows))\n",
    "axes = axes.flatten() if n_images > 1 else [axes]\n",
    "\n",
    "for i, filename in enumerate(spm_files):\n",
    "    afm_file = os.path.join(folder_path, f\"{filename}_corrected.csv\")\n",
    "    label_file = os.path.join(folder_path, f\"{os.path.splitext(filename)[0]}_categorised.csv\")\n",
    "\n",
    "    # Load AFM height data (in nm)\n",
    "    afm_data = pd.read_csv(afm_file, header=None).values\n",
    "\n",
    "    # Perform local thresholding\n",
    "    block_size = 19  # px\n",
    "    # offset_nm = 1  # nm\n",
    "    offset_nm = best_offsets[i]  # Use the best offset found earlier\n",
    "    local_thresh = filters.threshold_local(afm_data, block_size=block_size, offset=offset_nm)\n",
    "    dark_spots = afm_data < local_thresh\n",
    "\n",
    "    # Find contours of dark spots\n",
    "    contours = measure.find_contours(dark_spots, level=0.5)\n",
    "\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(afm_data, cmap='afmhot', vmin=-7, vmax=3)\n",
    "    ax.set_title(filename, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Draw contours\n",
    "    for contour in contours:\n",
    "        ax.plot(contour[:, 1], contour[:, 0], color='blue', linewidth=0.8)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7eb0cd",
   "metadata": {},
   "source": [
    "## Plot thresholding offset vs number of spots for each file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a70f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# Prepare figure layout\n",
    "n_images = len(spm_files)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_images / n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 14 / n_cols * n_rows))\n",
    "axes = axes.flatten() if n_images > 1 else [axes]\n",
    "\n",
    "\n",
    "for i, filename in enumerate(spm_files):\n",
    "    afm_file = os.path.join(folder_path, f\"{filename}_corrected.csv\")\n",
    "    label_file = os.path.join(folder_path, f\"{os.path.splitext(filename)[0]}_categorised.csv\")\n",
    "\n",
    "    label_data = pd.read_csv(label_file)\n",
    "\n",
    "    if i == 0:\n",
    "        all_labels = label_data\n",
    "    else:\n",
    "        all_labels = pd.concat([all_labels, label_data], ignore_index=True)\n",
    "\n",
    "    num_labels = len(label_data)\n",
    "\n",
    "    # Load AFM height data (in nm)\n",
    "    afm_data = pd.read_csv(afm_file, header=None).values\n",
    "\n",
    "    # Perform local thresholding\n",
    "    block_size = 19  # px\n",
    "\n",
    "    offsets = [x/10 for x in list(range(1, 30))]  # Offsets from 0.1 to 3.0 nm\n",
    "\n",
    "    num_spots_list = []\n",
    "\n",
    "    for offset_nm in offsets:\n",
    "        local_thresh = filters.threshold_local(afm_data, block_size=block_size, offset=offset_nm)\n",
    "        dark_spots = afm_data < local_thresh\n",
    "        # Count the number of dark spots\n",
    "        labeled_spots, num_spots = ndi.label(dark_spots)\n",
    "\n",
    "        # Calculate the sizes of all labeled spots\n",
    "        spot_sizes = np.bincount(labeled_spots.flat)[1:]\n",
    "\n",
    "        # Count spots that are between 6 and 199 pixels (inclusive)\n",
    "        num_spots = np.sum((spot_sizes >= 6) & (spot_sizes <= 199))\n",
    "\n",
    "        num_spots = min(num_spots,200)\n",
    "        num_spots_list.append(num_spots)\n",
    "\n",
    "    axes[i].plot(offsets, num_spots_list, label=filename)\n",
    "    axes[i].hlines(y=num_labels, xmin=0, xmax=offsets[-1], linestyles='dashed', colors='red', label=f'Labels: {num_labels}')\n",
    "    axes[i].set_title(filename, fontsize=10)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ffa2b",
   "metadata": {},
   "source": [
    "## Automatic Offset Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0540dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "from skimage import filters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def find_best_offset_first_derivative_peak(offsets, num_spots_list, peak_prominence=1.0, drop_threshold=20):\n",
    "    \"\"\"\n",
    "    Finds the best offset based on the first local maximum in the smoothed derivative.\n",
    "    - peak_prominence: how prominent a derivative peak must be to count\n",
    "    - drop_threshold: fraction of the peak height where we consider it to have 'dropped back down'\n",
    "    Ignores offsets where num_spots is NaN or zero.\n",
    "    \"\"\"\n",
    "    offsets = np.array(offsets)\n",
    "    num_spots = np.array(num_spots_list, dtype=float)\n",
    "\n",
    "    # Mask invalid\n",
    "    valid_mask = (~np.isnan(num_spots)) & (num_spots > 0)\n",
    "    if not np.any(valid_mask):\n",
    "        return None, None, None\n",
    "\n",
    "    offsets_valid = offsets[valid_mask]\n",
    "    num_spots_valid = num_spots[valid_mask]\n",
    "\n",
    "    # Compute and smooth derivative\n",
    "    deriv = np.diff(num_spots_valid) / np.diff(offsets_valid)\n",
    "    deriv_smooth = np.convolve(deriv, np.ones(5)/5, mode='same')\n",
    "\n",
    "    # Find peaks in the derivative\n",
    "    peaks, _ = find_peaks(deriv_smooth, prominence=peak_prominence)\n",
    "    if len(peaks) == 0:\n",
    "        return offsets_valid, num_spots_valid, (\"no_peak\", offsets_valid[np.argmax(num_spots_valid)], deriv_smooth)\n",
    "\n",
    "    first_peak_idx = peaks[0]\n",
    "    peak_height = deriv_smooth[first_peak_idx]\n",
    "\n",
    "    print('First peak height:', peak_height)\n",
    "\n",
    "    # Find the right edge where derivative drops below threshold fraction of peak height\n",
    "    right_edge_idx = first_peak_idx\n",
    "    print('Looking for next point where derivative < ', -drop_threshold + peak_height)\n",
    "    for j in range(first_peak_idx + 1, len(deriv_smooth)):\n",
    "        \n",
    "        if deriv_smooth[j] < -drop_threshold + peak_height:\n",
    "            right_edge_idx = j\n",
    "            print('Found the point at: ', deriv_smooth[j])\n",
    "            break\n",
    "\n",
    "    best_offset = offsets_valid[right_edge_idx]\n",
    "    return offsets_valid, num_spots_valid, (\"derivative_peak\", best_offset, deriv_smooth)\n",
    "\n",
    "\n",
    "def find_best_offset_plateau(offsets, num_spots_list, deriv_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Try to find the right edge of a plateau in the num_spots curve.\n",
    "    Fallback: return knee point if plateau not found.\n",
    "    Ignores offsets where num_spots is NaN or zero.\n",
    "    \"\"\"\n",
    "    offsets = np.array(offsets)\n",
    "    num_spots = np.array(num_spots_list, dtype=float)\n",
    "\n",
    "    # Mask invalid (NaN or 0)\n",
    "    valid_mask = (~np.isnan(num_spots)) & (num_spots > 0)\n",
    "    if not np.any(valid_mask):\n",
    "        return None, None, None  # No valid points\n",
    "\n",
    "    offsets_valid = offsets[valid_mask]\n",
    "    num_spots_valid = num_spots[valid_mask]\n",
    "\n",
    "    # Compute discrete derivative\n",
    "    deriv = np.diff(num_spots_valid) / np.diff(offsets_valid)\n",
    "    \n",
    "    # Smooth derivative\n",
    "    deriv_smooth = np.convolve(deriv, np.ones(10)/10, mode='same')\n",
    "    \n",
    "    # Plateau detection\n",
    "    plateau_idx = np.where(np.abs(deriv_smooth) < deriv_threshold)[0]\n",
    "\n",
    "    if len(plateau_idx) > 0:\n",
    "        # Group consecutive indices\n",
    "        groups = np.split(plateau_idx, np.where(np.diff(plateau_idx) != 1)[0] + 1)\n",
    "        # Pick the longest low-derivative segment\n",
    "        longest = max(groups, key=len)\n",
    "        best_idx = longest[-1]\n",
    "        return offsets_valid, num_spots_valid, (\"plateau\", offsets_valid[best_idx], deriv_smooth)\n",
    "\n",
    "    # Fallback: Knee detection\n",
    "    y = num_spots_valid\n",
    "    x = offsets_valid\n",
    "    x_norm = (x - x.min()) / (x.max() - x.min())\n",
    "    y_norm = (y - y.min()) / (y.max() - y.min())\n",
    "    distances = np.abs(y_norm - (y_norm[0] + (y_norm[-1] - y_norm[0]) * x_norm))\n",
    "    knee_idx = np.argmax(distances)\n",
    "    return offsets_valid, num_spots_valid, (\"knee\", offsets_valid[knee_idx], deriv_smooth)\n",
    "\n",
    "# ---- Main loop ----\n",
    "plt.close('all')\n",
    "n_images = len(spm_files)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_images / n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 14 / n_cols * n_rows))\n",
    "axes = axes.flatten() if n_images > 1 else [axes]\n",
    "\n",
    "block_size = 19  # px\n",
    "offsets = [x / 40 for x in range(4, 120)]\n",
    "\n",
    "best_offsets = []\n",
    "\n",
    "for i, filename in enumerate(spm_files):\n",
    "    afm_file = os.path.join(folder_path, f\"{filename}_corrected.csv\")\n",
    "    label_file = os.path.join(folder_path, f\"{os.path.splitext(filename)[0]}_categorised.csv\")\n",
    "\n",
    "    label_data = pd.read_csv(label_file)\n",
    "    num_labels = len(label_data)\n",
    "\n",
    "    afm_data = pd.read_csv(afm_file, header=None).values\n",
    "\n",
    "    num_spots_list = []\n",
    "\n",
    "    for offset_nm in offsets:\n",
    "        local_thresh = filters.threshold_local(afm_data, block_size=block_size, offset=offset_nm)\n",
    "        dark_spots = afm_data < local_thresh\n",
    "        labeled_spots, _ = ndi.label(dark_spots)\n",
    "        spot_sizes = np.bincount(labeled_spots.flat)[1:]\n",
    "        num_spots = np.sum((spot_sizes >= 6) & (spot_sizes <= 199))\n",
    "        if num_spots > 200:\n",
    "            num_spots_list.append(np.nan)  # invalid\n",
    "        else:\n",
    "            num_spots_list.append(num_spots if num_spots > 0 else np.nan)\n",
    "\n",
    "    ax = axes[i]\n",
    "    # offsets_valid, num_spots_valid, result = find_best_offset_plateau(offsets, num_spots_list)\n",
    "    offsets_valid, num_spots_valid, result = find_best_offset_first_derivative_peak(offsets, num_spots_list)\n",
    "\n",
    "    \n",
    "    if result is None:\n",
    "        ax.set_title(f\"{filename}\\nNo valid points\")\n",
    "        continue\n",
    "\n",
    "    method, best_offset, deriv_smooth = result\n",
    "\n",
    "    best_offsets.append(best_offset)\n",
    "\n",
    "    # Plot number of spots\n",
    "    ax.plot(offsets, num_spots_list, label='Num spots', color='blue')\n",
    "    ax.axvline(best_offset, color='orange', linestyle='--', label=f'{method} ({best_offset:.2f} nm)')\n",
    "    ax.hlines(y=num_labels, xmin=0, xmax=offsets[-1], linestyles='dashed', colors='red', label=f'Labels: {num_labels}')\n",
    "\n",
    "    # Plot derivative on secondary y-axis\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(offsets_valid[:-1], deriv_smooth, color='green', linestyle='-', label='Derivative (smoothed)')\n",
    "    ax2.set_ylabel('Î”spots / Î”offset', color='green')\n",
    "\n",
    "    ax.set_title(filename, fontsize=10)\n",
    "    ax.set_xlabel(\"Offset (nm)\")\n",
    "    ax.set_ylabel(\"Spot count\")\n",
    "    ax.legend(fontsize=6, loc='upper left')\n",
    "    ax2.legend(fontsize=6, loc='upper right')\n",
    "\n",
    "    # if i >= 6:\n",
    "    #     break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4ad9b",
   "metadata": {},
   "source": [
    "## Plot labelled AFM images individually (Viridis colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40cc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "for filename in spm_files:\n",
    "    afm_file = os.path.join(folder_path, f\"{filename}_corrected.csv\")\n",
    "    label_file = os.path.join(folder_path, f\"{os.path.splitext(filename)[0]}_categorised.csv\")                    \n",
    "\n",
    "    afm_data = pd.read_csv(afm_file).values\n",
    "    labels = pd.read_csv(label_file)\n",
    "\n",
    "    # Display the shape of the loaded data\n",
    "    print(f\"AFM data shape: {afm_data.shape}\")\n",
    "    print(f\"Labels data shape: {labels.shape}\")\n",
    "\n",
    "    # Visualize the AFM data\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(afm_data, cmap='viridis')\n",
    "    plt.title('AFM Topography')\n",
    "    plt.colorbar(label='Height (nm)')\n",
    "    plt.savefig('afm_topography.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Visualize the AFM data with labels overlaid\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(afm_data, cmap='viridis')\n",
    "    plt.title('AFM Topography with Labeled Pits')\n",
    "    plt.colorbar(label='Height (nm)')\n",
    "\n",
    "    # Add circles for each labeled pit\n",
    "    for index, row in labels.iterrows():\n",
    "        # We use a color that will be visible on the viridis map.\n",
    "        # The category can be used to color-code the circles if desired.\n",
    "        color = 'r' if row['category'] == 'large' else 'orange'\n",
    "        circle = Circle((row['x'], 511-row['y']), sqrt(row['size'])*2, color=color, fill=False, linewidth=2)\n",
    "        plt.gca().add_patch(circle)\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "# plt.savefig('afm_topography_with_labels.png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952960b",
   "metadata": {},
   "source": [
    "## Create overlapping histograms for 'size' grouped by 'category'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178525f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['large','small']\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for category in categories:\n",
    "    subset = all_labels[all_labels['category'] == category]\n",
    "    plt.hist(subset['size']*61, bins=20, alpha=0.5, label=category)\n",
    "\n",
    "plt.xlabel('Pit Size ($nm^2$)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Overlapping Histograms of Pit Size by Category')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4319b89",
   "metadata": {},
   "source": [
    "## CNN Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40889012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# -------------------------\n",
    "# Parameters\n",
    "# -------------------------\n",
    "cutout_size = 19\n",
    "half_size = cutout_size // 2\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# -------------------------\n",
    "# Data extraction\n",
    "# -------------------------\n",
    "for filename in spm_files:\n",
    "    afm_file = os.path.join(folder_path, f\"{filename}_corrected.csv\")\n",
    "    label_file = os.path.join(folder_path, f\"{os.path.splitext(filename)[0]}_categorised.csv\")\n",
    "\n",
    "    afm_data = pd.read_csv(afm_file, header=None).values\n",
    "    labels = pd.read_csv(label_file)\n",
    "\n",
    "    # Spot cutouts (label=1)\n",
    "    for _, row in labels.iterrows():\n",
    "        if row['category'] not in ['large', 'small']:\n",
    "            continue\n",
    "        cx, cy = int(row['x']), int(row['y'])\n",
    "        # Flip y for correct orientation\n",
    "        cy = afm_data.shape[0] - 1 - cy\n",
    "        if cx - half_size < 0 or cx + half_size >= afm_data.shape[1] or cy - half_size < 0 or cy + half_size >= afm_data.shape[0]:\n",
    "            continue  # skip if too close to border\n",
    "        patch = afm_data[cy - half_size:cy + half_size + 1, cx - half_size:cx + half_size + 1]\n",
    "        X.append(patch)\n",
    "        y.append(1)\n",
    "\n",
    "        # if row['category'] == 'large':\n",
    "        #     y.append(1)\n",
    "        # else:\n",
    "        #     y.append(0)\n",
    "\n",
    "    ## ---- Uncomment below lines for spot/no-spot training (also remove the if 'large' statement above)\n",
    "\n",
    "    # No-spot cutouts (label=0)\n",
    "    # Sample random positions far from labeled spots\n",
    "    mask = np.zeros_like(afm_data, dtype=bool)\n",
    "    for _, row in labels.iterrows():\n",
    "        rr, cc = int(row['y']), int(row['x'])\n",
    "        rr = afm_data.shape[0] - 1 - rr\n",
    "        yy, xx = np.ogrid[:afm_data.shape[0], :afm_data.shape[1]]\n",
    "        dist = np.sqrt((xx - cc)**2 + (yy - rr)**2)\n",
    "        mask[dist <= half_size] = True\n",
    "\n",
    "    n_neg_samples = len(labels)  # balance positives and negatives\n",
    "    neg_coords = np.argwhere(~mask)\n",
    "    np.random.shuffle(neg_coords)\n",
    "    for ny, nx in neg_coords[:n_neg_samples]:\n",
    "        if nx - half_size < 0 or nx + half_size >= afm_data.shape[1] or ny - half_size < 0 or ny + half_size >= afm_data.shape[0]:\n",
    "            continue\n",
    "        patch = afm_data[ny - half_size:ny + half_size + 1, nx - half_size:nx + half_size + 1]\n",
    "        X.append(patch)\n",
    "        y.append(0)\n",
    "\n",
    "    ## ----- end spot/no-spot section\n",
    "\n",
    "# Convert to arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Normalise height values\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "X = X[..., np.newaxis]  # add channel dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d124fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Train/test split\n",
    "# -------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# -------------------------\n",
    "# CNN model\n",
    "# -------------------------\n",
    "# model = models.Sequential([\n",
    "#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(cutout_size, cutout_size, 1)),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dense(1, activation='sigmoid')  # binary classification\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# -------------------- OLD Model -----------\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(cutout_size, cutout_size, 1)),  # Updated input_shape to (15, 15, 3)\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# -------------------------\n",
    "# Training\n",
    "# -------------------------\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32,\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "# -------------------------\n",
    "# Plot loss curves\n",
    "# -------------------------\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Predictions\n",
    "# -------------------------\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# -------------------------\n",
    "# Confusion matrix\n",
    "# -------------------------\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['Small', 'Large'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Metrics\n",
    "# -------------------------\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Predictions\n",
    "# -------------------------\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# -------------------------\n",
    "# Confusion matrix\n",
    "# -------------------------\n",
    "labels = ['Small', 'Large']\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)\n",
    "\n",
    "# -------------------------\n",
    "# Metrics\n",
    "# -------------------------\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average='binary', zero_division=0)\n",
    "recall = recall_score(y_val, y_pred, average='binary', zero_division=0)\n",
    "f1 = f1_score(y_val, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "# Per-class metrics\n",
    "class_prec = precision_score(y_val, y_pred, average=None, zero_division=0)\n",
    "class_rec = recall_score(y_val, y_pred, average=None, zero_division=0)\n",
    "class_f1 = f1_score(y_val, y_pred, average=None, zero_division=0)\n",
    "\n",
    "# Build annotation text\n",
    "textstr = f\"Overall:\\n\" \\\n",
    "          f\"Acc: {accuracy:.3f}\\n\" \\\n",
    "          f\"Prec: {precision:.3f}\\n\" \\\n",
    "          f\"Rec: {recall:.3f}\\n\" \\\n",
    "          f\"F1: {f1:.3f}\\n\\n\" \\\n",
    "          f\"Per-class:\\n\" \\\n",
    "          f\"{labels[0]} - P:{class_prec[0]:.2f}, R:{class_rec[0]:.2f}, F1:{class_f1[0]:.2f}\\n\" \\\n",
    "          f\"{labels[1]} - P:{class_prec[1]:.2f}, R:{class_rec[1]:.2f}, F1:{class_f1[1]:.2f}\"\n",
    "\n",
    "# Add text box to plot\n",
    "ax.text(1.05, 0.5, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='center', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7446833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Predictions\n",
    "# -------------------------\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# -------------------------\n",
    "# Confusion matrix\n",
    "# -------------------------\n",
    "labels = ['Small', 'Large']\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)\n",
    "\n",
    "# -------------------------\n",
    "# Metrics\n",
    "# -------------------------\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average='binary', zero_division=0)\n",
    "recall = recall_score(y_val, y_pred, average='binary', zero_division=0)\n",
    "f1 = f1_score(y_val, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "# Per-class metrics\n",
    "class_prec = precision_score(y_val, y_pred, average=None, zero_division=0)\n",
    "class_rec = recall_score(y_val, y_pred, average=None, zero_division=0)\n",
    "class_f1 = f1_score(y_val, y_pred, average=None, zero_division=0)\n",
    "\n",
    "# -------------------------\n",
    "# Create metrics table data\n",
    "# -------------------------\n",
    "table_data = [\n",
    "    [\"Metric\", \"Overall\", labels[0], labels[1]],\n",
    "    [\"Accuracy\", f\"{accuracy:.3f}\", \"\", \"\"],\n",
    "    [\"Precision\", f\"{precision:.3f}\", f\"{class_prec[0]:.3f}\", f\"{class_prec[1]:.3f}\"],\n",
    "    [\"Recall\", f\"{recall:.3f}\", f\"{class_rec[0]:.3f}\", f\"{class_rec[1]:.3f}\"],\n",
    "    [\"F1 Score\", f\"{f1:.3f}\", f\"{class_f1[0]:.3f}\", f\"{class_f1[1]:.3f}\"]\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Add table below the confusion matrix\n",
    "# -------------------------\n",
    "ax_table = plt.gcf().add_axes([0.15, -0.35, 0.7, 0.25])  # position: [left, bottom, width, height]\n",
    "ax_table.axis('off')\n",
    "\n",
    "table = ax_table.table(\n",
    "    cellText=table_data,\n",
    "    cellLoc='center',\n",
    "    colLabels=None,\n",
    "    loc='center'\n",
    ")\n",
    "\n",
    "# Style table\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 1.5)\n",
    "\n",
    "# Color header row\n",
    "for col in range(len(table_data[0])):\n",
    "    table[(0, col)].set_facecolor('#1f77b4')\n",
    "    table[(0, col)].set_text_props(color='white', weight='bold')\n",
    "\n",
    "# Color first column\n",
    "for row in range(len(table_data)):\n",
    "    table[(row, 0)].set_facecolor('#c6dcee')\n",
    "    table[(row, 0)].set_text_props(weight='bold')\n",
    "\n",
    "plt.subplots_adjust(bottom=0.1)  # make space for table\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions (probabilities)\n",
    "y_pred_probs = model.predict(X_val)\n",
    "\n",
    "# Convert to binary labels\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Find incorrect predictions\n",
    "incorrect_indices = np.where(y_pred != y_val)[0]\n",
    "correct_indices = np.where(y_pred == y_val)[0]\n",
    "\n",
    "print(f\"Number of misclassified images: {len(incorrect_indices)}\")\n",
    "\n",
    "labels = ['No Spot', 'Spot']\n",
    "\n",
    "# Display them\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, idx in enumerate(correct_indices[[0,1,3,4]]):\n",
    "    plt.subplot(2, 2, i+1)  # 5x5 grid, adjust as needed\n",
    "    plt.imshow(X_val[idx].squeeze(), cmap='gray')\n",
    "    plt.title(f\"True: {labels[y_val[idx]]}, Pred: {labels[y_pred[idx]]}\", fontsize=10)\n",
    "    plt.axis('off')\n",
    "    if i >= 3:  # limit to first 25 for display\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d729f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0,:,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
